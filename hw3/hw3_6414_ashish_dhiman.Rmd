---
title: "HW3_ISYE6414_ashish_dhiman"
author: "Ashish Dhiman | adhiman9@gatech.edu"
date: "2022-09-12"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
setwd("~/data_projects/fall22_hw/isye6414_hw/hw3")
```

## Read Data and Summary

```{bash}
head -5 ./6414_HW3_Clean.csv
```

```{r}
df_demand_price = read.table(file ="./6414_HW3_Clean.csv", sep=",",header=TRUE)
head(df_demand_price)
dim(df_demand_price)
summary(df_demand_price)
```

## Question 1: Scatter Plot

```{r}
title_i = "Demand (in hundred thousands) vs Price Delta (in USD)"
plot(x=df_demand_price$PriceDif, y=df_demand_price$Demand, type ="p",main = title_i)
```

$\color{blue}{\text{From the above plot, a linear realationship between Demand and Price Difference is apparent}}$

$\color{blue}{\text{The strength of the linear realtionship can also be tested with corealtion between x and y}}$

```{r}
cor_xy = cor(df_demand_price$Demand,df_demand_price$PriceDif)
print (paste("Corealtion on full data:",round(cor_xy,2)))
```

$\color{blue}{\text{A corelation of 0.89 is pretty significant and further supports strong linear realtionship between x and y}}$


## Question 2: Simple Linear Regression and Intercepts

```{r}
#Fit SLR
slr_model <- lm(Demand ~ PriceDif, data = df_demand_price)
slr_model

#Superpositioning regression line on 
ggplot(df_demand_price, aes(PriceDif, Demand)) + #aes(x,y)
  geom_point() +
  stat_smooth(method = lm, se = FALSE)
```

```{r}
summary(slr_model)
```

From above summary we have:

$$
\hat{\beta_0} = 7.81409\\
\hat{\beta_1} = 2.66521\\
\hat{\sigma} = 0.3166\\
\hat{se(\beta_0)} = 0.07988\\
\hat{se(\beta_1)} = 0.25850\\
$$


### Question 3, 95% CI for $\hat{\beta_1}$

$$
CI\ for\ \hat{\beta_1}\ at\ (1-\alpha)\% = \hat{\beta_1} \pm t_{\frac{\alpha}{2},n-2}\sqrt{\frac{MSE}{S_{xx}}}
$$

```{r}
#In R this is given as:
confint(slr_model,level = 0.95)
print (paste("95% CI for beta1 is (2.135702,3.194727)"))
print (paste("Length of CI in terms of sd:",round((3.194727-2.135702)/0.25850,2)))
```
From 95% CI we can ascertain that  beta1 lies within (2.135702,3.194727) range with 95% probability.
B'cos the above CI is taken from t -distribution, which is fatter at tails (relative to normal), we get 4.1sd compared to 4 for normal.


### Question 4: Hypothesis Test on if x is statistically significant

For predictor x to be statistically significant, $\beta_1$ should not be 0. Let us conclude a Hypothesis Test for it:

Null Hypothesis = $H_0: \hat{\beta_1}$

Alternate Hypothesis = $H_1: \hat{\beta_1} \neq 0$

Then from the model we have, Test Statistic = $\frac{\hat{\beta_1} - 0}{\sqrt{\frac{MSE}{S_{xx}}}}$ = 10.31

```{r}
print ("Critical t value, for alpha 5%:")
qt(p=0.975,df=28)
qt(p=0.025,df=28)
```

Here we are performing a two tailed test for our Null Hypothesis using $\alpha = 5\%$. To reject $H_0$ we want, the test statistic (i.e. the t value) to be:

$$
t \in (-\inf,-2.048407) \cup (2.048407,inf)
$$

In this case our t value of 10.31 lies in the rejection region. Therefore we can conclude that at alpha = 5%, beta1 is not 0, and is statistically significant. In other words, given the current data there is 5% chance that $\beta_1$ is 0. This also implies that there is support for linear relationship between x & y, else there would not be statistical evidence to refute $H_0 : \beta_0 = 0$

```{r}
summary(slr_model)$coefficients
```

### Question 5: p value for $\beta_0$

From model summary the p value here is $< 4.851255 * 10^{-37}$. This is very strong(or low) p-value and for almost any typical alpha level of 1%, 5%, 10%, we have statistical evidence to reject the null hypothesis $H_0: beta_0 = 0$


### Question 6: p value for $\beta_1$

From model summary the p value here is $(4.88 * 10^{-11})$

For Null Hypothesis = $H_0: \hat{\beta_1}$

Because p-value is less than 10%, 5% and 0.5%, we can reject null hypothesis at all these alpha levels.

Now we have such a low value, this implies given this data it is highly highly improbable ( 1 in $10^{11}$) times that we fail to reject H0 when H0 is correct. In other words, we have support for very very strong linear relationship between x & y.


### Question 7: point estimate & 95% CI for mean demand value for $x = 0.1$

We have to find $E[\hat{y}|x=0.1]$

```{r}
test = data.frame(PriceDif = 0.1)
predict.lm(slr_model, test, interval = "confidence", level = 0.95)
```

From above the point estimate is 8.080609, and 95% CI is (7.947878,8.21334)


### Question 8: point estimate & 95% prediction interval for actual demand value for $x = 0.1$

```{r}
predict.lm(slr_model, test, interval = "predict", level = 0.95)
```

From above the point estimate in this case is 8.080609, and 95% CI is (7.418719,8.7425)

```{r}
half_length_ci = (8.21334-7.947878)/2
half_length_pi = (8.7425-7.418719)/2

print (paste("half Length CI",half_length_ci))
print (paste("half Length PI",half_length_pi))
print (half_length_pi/half_length_ci)
```

**Because prediction variance has extra 1 in the variance term, prediction interval is 5 times larger than CI.**


### Question 9: point estimate & 95% CI for mean demand value for $x = 0.25$

```{r}
test2 = data.frame(PriceDif = 0.25)
predict.lm(slr_model, test2, interval = "confidence", level = 0.95)
```

```{r}
half_length_ci2 = (8.600362-8.36042)/2
print(paste("mean x and CI2 = ",mean(df_demand_price$PriceDif),half_length_ci2))
```

Because 0.25 is closer to mean x vs 0.1, the half length of ci in this is case smaller compared to ci for x=0.1. This is because the CI term has a factor of $x_i - \bar{x}$


### Question 10: Derivation

For $\hat{\beta_0} = 0$, then we have:
$$
\hat{y_i} = \hat{\beta_1} x_i ; \quad then\ SSE = \sum(y_i - \hat{y_i})^2 =(y_i - \hat{\beta_1}.x_i)^2
$$
We want to find beta1 which minimises SSE. So we take derivative wrt beta1 and equate it to 0.
$$
\frac{\partial SSE}{\partial\beta_1} = \sum_i^n[2(y_i - \hat{\beta_1}x_i).(-x_i)] = 0
$$
$$
\implies \sum_i^n x_i.y_i = \hat{\beta_1}.\sum_i^n x_i^2 \quad or\ \hat{\beta_1} = \frac{\sum_i^n x_i.y_i}{\sum_i^n x_i^2}
$$
Hence Proved